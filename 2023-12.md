### Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation  

这篇文章提出 zeroshot text-guided video-to-video translation 

+ key frame translation   
  + 扩散模型生成关键帧
  + 关键帧跨帧约束
+ full video translation 
  + 渲染好的关键帧传播生成其他帧
  +  temporal-aware patch matching and frame blending  

现有的 text-guided video diffusion models  存在的问题：

+ training a video model on large-scale video data  [ Imagen video: High definition video generation with diffusion models. ]（计算资源要求高，不能即插即用）
+ fine-tune image models on a single video  [ Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. ]（难以生成长视频，在单个视频上微调容易过拟合导致原先模型的表现不佳）
+ zero-shot methods  with cross-frame constaints imposed on the latent features for temporal consistency [ Text2video-zero: Text-toimage diffusion models are zero-shot video generators.  ] （现有的跨帧约束局限于全局风格，对于细节的一致性控制不行）

Related Work：

+ 依赖于大规模视频数据集训练的方法：

  + Video Diffusion Model：扩展图像模型中的 2D U-Net 为 space-time UNet
  + Imagen Video：级联的 spatial and temporal 视频超分模型
  + Dreamix：基于 Imagen Video 的视频编辑
  + Make-A-Video：在无监督条件下使用视频数据来学习运动状态

+ 依赖于 fine-tuning 的方法：

  + Tune-A-Video：cross-frame attention，在单个视频上 fine-tuning 学习相关动作
  + Edit-A-Video, Video-P2P, and vid2vid-zero：使用 Null-Text Inversion（Null-text inversion for editing real images using guided diffusion models.  ）来保存未编辑的区域

+ Zero-Shot 方法，更适合预训练扩散模型变量比如 InstructPix2Pix 或者 ControlNet 来获得 depth and edges 这些更加灵活的状态：

  + FateZero：基于 Prompt2Prompt 获得编辑区域的 mask，在编辑前后混合 attention features
  + Text2Video-Zero：translates the latent to directly simulate motions  
  + Pix2Video：将当前帧和前帧的隐变量做匹配

  以上的 Zero-Shot 方法都用到了 cross-frame attention，以及 early-step latent fusion 来提高时间一致性。

  然而，只能保证图像级别的风格一致，对于像素级别的细节处理不到位。























